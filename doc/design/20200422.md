# Notes (call 22.4.2020)
## 1 ‘Coordinator’ performing operation on N ‘non-coordinator’ nodes

- Coordinator is the one being called to do backup, clean backups, restore
- Coordinator responsibility spans only the lifetime of a single cluster wide operation (such as backup, or cleanup)
- Central locking by calling all nodes
    - This prevents e.g. sudden new node trying to start new backup from going anywhere
- Acquires central lock => does stuff => finishes => releases central lock
    - In case of failure, just cascade failure (but may leave e.g. already uploaded parts next operation will be cheaper and faster)
    - Lock should have ttl, and should be reacquired well before ttl expiration
    - Timeout = backup failed

## Error cases
- Should survive process restart of non-coordinator nodes’ software
- Design should allow coordinator tasks to be resumed as well, but initial implementation doesn’t have to survive that ( we will just do it later )
- When configuration changes (for e.g. coordinator), should abort ongoing
operations by default


## Operations
- List backups
- Clean backups
- Backup
- Restore (from specified backup)

## Each operation should be divisible to stages
- And per-stage on-coordinator / all-node steps are implemented by product specific plugins
- Essentially state machine on coordinator with state per stage
    - And cross-stage state for the operation (at least on coordinator; we may or may not make whole pipeline available to the nodes performing single operations)
- Actual content being backed up should be property of the actual plugin; some centralized utils for e.g. directory hierarchy upload could be available


# HTTP REST endpoints
- On nodes
- Callback URL on coordinator
- Command REST API on coordinator is optional from design point of view
  probably preferable to something else
